import requests
import pandas as pd


URL = 'https://realpython.github.io/fake-jobs/'

response = requests.get(URL)


type(response)


response.status_code


from bs4 import BeautifulSoup as BS


soup = BS(response.text)


soup.find('h2')


type(soup.find('h2'))


soup.find('h2').text


jobs = soup.find_all('h2')


job_list = [job.text for job in jobs]


companies = soup.find_all('h3')


company_list = [company.text for company in companies]


locations = soup.find_all('p', attrs = {'class' : 'location'})


location_list = [location.text.strip() for location in locations]


times = soup.find_all('time')


time_list = [time.text for time in times]


fake_jobs = pd.DataFrame({'title' : job_list, 'company' : company_list, 'location' : location_list, 'date' : time_list})


a_tags = soup.find_all('a')


links = [a_tag['href'] for a_tag in a_tags]


counter = 0
link_list=[]

for link in links:
    counter+=1

    if counter%2 ==0:
        link_list.append(link)


link_list2 = [link for ind, link in enumerate(links) if ind %2 != 0]


fake_jobs['apply_1'] = link_list


link_starter = 'https://realpython.github.io/fake-jobs/jobs/'
link_ender = '.html'


counter = 0

for job in job_list:
    job = job.replace(' ','-')
    job = job.replace('(', '')
    job = job.replace(')','')
    job = job.lower()
    job = job.replace(',', '')
    job = job.replace('/', '-')
    print(link_starter + job + '-' + str(counter) + link_ender)
    counter+=1


link_list_3 = []

for ind, job in enumerate(job_list):
    job = job.replace(' ','-')
    job = job.replace('(', '')
    job = job.replace(')','')
    job = job.lower()
    job = job.replace(',', '')
    job = job.replace('/', '-')
    link_list_3.append(link_starter + job + '-' + str(ind) + link_ender)


fake_jobs['apply_2'] = link_list_3


for ind, row in fake_jobs.iterrows():
    if row.apply_1 != row.apply_2:
        print(ind)


fake_jobs.apply_1.equals(fake_jobs.apply_2)


link_list == link_list_3


URL = 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'


requests.get(URL)


soup = BS(response.text)


soup.find_all('p')[1].text


desc_list = []

for ind, row in fake_jobs.iterrows():
    new_URL = row.apply_1
    response = requests.get(new_URL)
    soup = BS(response.text)
    desc_list.append(soup.find_all('p')[1].text)


def pull_desc(my_url):
    response = requests.get(my_url)
    soup = BS(response.text)
    return soup.findALL('p')[1].text


fake_jobs['job_description'] = fake_jobs.apply_1.apply(pull_desc)
